# Experiment 1: ME Effect and Vocabulary Size

In Experiment 1, we test the prediction that children with larger vocabularies should show a strong ME effect by measuring vocabulary size in a large sample of children across multiple ages who also completed the ME task. We find that vocabulary size is a strong predictor of the strength of the ME effect across development and that vocabulary size predicts more variance than developmental age.

## Methods

We report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures in the study. Our hypotheses and analysis plan were pre-registered (\url{https://osf.io/tt29f/register/5771ca429ad5a1020de2872e}), and we note below where our analyses diverged from this pre-registration.

### Participants

```{r read_in_data}
EXP1_DATA_PATH <- here("data/1_exp_data.csv")
exp1 <- read_csv(EXP1_DATA_PATH) %>%
  select(sub_id, age_months, gender, english, 
         exclude2, prop_correct_vocab, trial_type, correct, 
         start_time, end_time, resp_start_time, object1)  
```

```{r complete_trials_from_target_kids}
# filter to kids in target age range and completed all trials
NUM_TRIALS <- 19
good_counts <- exp1 %>%
  count(sub_id) %>%
  filter(n == NUM_TRIALS) 

exp1_complete <- exp1 %>%
  filter(age_months >= 24 & age_months <= 48,
         sub_id %in% good_counts$sub_id) %>%
  mutate(age_bin = as.factor(case_when(age_months >= 36 ~ "3-yo", 
                             TRUE ~ "2-yo"))) 

total_sample_size_by_age<- exp1_complete %>%
  distinct(sub_id, .keep_all = T) %>%
  filter(sub_id %in% good_counts$sub_id) %>%
  count(age_bin)
```

```{r do_exclusions}
# prop_correct C-NF > .5
good_controls <- exp1_complete %>%
  filter(trial_type == "C-NF") %>%
  group_by(sub_id) %>%
  summarize(prop_correct = sum(correct) / n())  %>%
  filter(prop_correct >= .5) 

# english input >= 75
good_language <- exp1_complete %>%
  distinct(sub_id, .keep_all = T) %>%
  filter(english >= 75) 

# final sample
final_sub_ids <- list(good_controls$sub_id,
                      good_language$sub_id) %>%
  accumulate(intersect) %>%
  last()

final_sample <- exp1_complete %>%
               filter(sub_id %in% final_sub_ids) 

n_girls <- final_sample %>%
  distinct(sub_id, .keep_all = T) %>%
  filter(gender == "female")

e1_demos <- final_sample %>%
  distinct(sub_id, .keep_all = T) %>%
  group_by(age_bin) %>%
  summarise(mean_age = round(mean(age_months),2), 
            n = n()) %>%
  mutate_at(vars(n, age_bin), as.character)

colnames(e1_demos) <- c("Age group", "Mean age (months)", "Sample size")
```

```{r, results = "asis"}
apa_table(e1_demos, caption = "Demographics of children in Experiment 1.")
```

We conducted a range of power analyses to determine our sample size and found that we needed a large sample size to estimate the unique effect of vocabulary on accuracy, since vocabulary and age tend to be highly correlated with each other. We registered our target sample size to be 80 2-year-olds and 80 3-year-olds. In total, `r sum(total_sample_size_by_age$n)` children completed the task (2-yo: *N* = `r filter(total_sample_size_by_age, age_bin == "2-yo") %>% pull(n)`; 3-yo: *N* = `r filter(total_sample_size_by_age, age_bin == "3-yo") %>% pull(n)`). We excluded participants who did not correctly answer at least half of the familiar noun control trials (*N* = `r sum(total_sample_size_by_age$n) - nrow(good_controls)`), as described in our pre-registration. In addition, there were `r sum(total_sample_size_by_age$n) - nrow(good_language)` children in our sample whose parents reported that they were exposed to English less than 75% of the time. We excluded these participants from our analysis since both previous work [e.g., @byers2009monolingual] and our meta-analysis has suggested that the ME effect is affected by multilingualism. Exclusions on the basis of language input were not described in our pre-registration analysis plan, but all analyses remain qualitatively the same when these children are included in the sample. Our final sample included `r length(unique(final_sample$sub_id))` children ($N_{female}$ = `r nrow(n_girls)`; see Table 2). 

### Stimuli

The ME task included color pictures of 14 novel objects (e.g., a funnel) and 24 familiar objects (e.g. a ball; see Appendix).  The novel words were the real 1-3 syllables labels for the unfamiliar objects (e.g., "funnel", "tongs", etc.; see Appendix). Items in the vocabulary assessment were a fixed set of 20 developmentally appropriate words from the Pearson Peabody Vocabulary Test [PPVT; see Appendix; @dunn1965peabody].  We selected words for our vocabulary assessment on the basis of pilot testing and age of acquisition data from the Wordbank database [@frank2017wordbank] with the goal of identifying words that would be challenging for children across the target age range. We developed our own very short, tablet-based assessment of vocabulary size because the complete PPVT would be prohibitively time consuming and the CDI could not be used with our full target age range.

### Design and Procedure
In order to test a large sample of children, we designed a short and simple testing procedure that could be conducted on a tablet in a museum setting.  Sessions took place individually in a small testing room away from the museum floor.  The experimenter first introduced the child to "Mr. Fox," a cartoon character who wanted to play a guessing game (see Fig. 3a). The experimenter explained that Mr. Fox would tell them the name of the object they had to find, so they had to listen carefully. Children then completed a series of 19 trials on an iPad, 3 practice trials followed by 16 experimental trials. In the practice trials, children were shown two familiar pictures (FF) on the tablet and asked to select one given a label (e.g. "Touch the ball!"). If the participant chose incorrectly on a practice trial, the audio would correct them and allow the participant to choose again. The audio was presented through the tablet speakers. 

```{r, fig.pos = "t!", fig.cap = "Example screenshots for an Experimental Novel-Familiar test trial in Experiment 1. On each test trial, Mr. Fox first appeared to get the child's attention (a). Next, an object appeared and was labeled through the tablet speakers ('It's a ball'; b). Two objects then appeared and children were asked to make a selection ('Touch the funnel'; c)."}
include_graphics("figs/fendle_fig1.png")
```

In the test phase, each test trial consisted of two screens: One presenting a single object and an unambiguous label (Fig. 3b), and another presenting two objects and a single label  (Fig. 3c). The child's task was to identify the referent on the second screen  (Fig. 3c). Within participants, we manipulated two features of the task: the target referent (Novel (Experimental) or Familiar (Control)) and the type of alternatives (Novel-Familiar or Novel-Novel; NF or NN). On novel referent trials (Experimental), children were expected to select a novel object via the ME inference. On familiar referent trials (Control), children were expected to select the correct familiar object. On Novel-Familiar trials, children saw a picture of a novel object and a familiar object (e.g. a funnel and a ball). On Novel-Novel trials, children saw pictures of two novel objects (e.g.\ a pair of tongs and a leek). The design features were fully crossed such that half of the trials were of each trial type  (Experimental-NF, Experimental-NN, Control-NF, Control-NN; Table 3). Trials were presented randomly, and children were only allowed to make one selection. 

\begin{table}[]
\centering
\caption{Design for each of the four trial types. "N" indicates a novel referent and "F" indicates a familiar referent. Each test trial involved two displays. The first display introduced an object and its label unambiguously; the second presented two objects and a single label and children were asked to identify the target referent. }
\label{my-label}
\begin{tabular}{|l|l|l|l|}
\hline
Trial Type   & Screen 1 Display & Screen 2 Display & Target (Audio) \\ \hline
Experimental & $F$                & $NF$                &$N$              \\ \hline
Experimental & $N_1$              & $N_1N_2$                 & $N_2$              \\ \hline
Control      & $F$                & $NF$              & $F$              \\ \hline
Control      & $N_1$                 & $N_1N_2$               &  $N_1$           \\ \hline
\end{tabular}
\end{table}


After the ME task, we measured children's vocabulary in a simple vocabulary assessment in which children were presented with four randomly selected images and prompted to choose a picture given a label. Children completed two practice trials followed by 20 test trials.  

### Data analysis
Selections on the ME task were coded as correct if the participant selected the familiar object on Control trials and the novel object on Experimental trials. We centered both age and vocabulary size for interpretability of coefficients. All models are logistic mixed effect models fit with the lme4 package in R [@R-lme4]. All ranges are 95% confidence intervals. Effect sizes are Cohen's *d* values.

## Results
```{r practice_trials}
practice_trials <- final_sample %>%
  filter(trial_type == "FF") %>%
  group_by(sub_id) %>%
  summarize(prop_correct = mean(correct)) %>%
  mutate(group = "temp") %>%
  group_by(group) %>%
  multi_boot_standard("prop_correct") 

print_practice <- get_pretty_mean(practice_trials$mean, 
                                  practice_trials$ci_lower, 
                                  practice_trials$ci_upper)
```

Participants completed the three practice trials (FF) with high accuracy, suggesting that they understood the task (*M* = `r print_practice`). 

```{r summary_stats}
mss <- final_sample %>%
  mutate(age_group = cut(age_months, 
                         breaks = c(24, 30, 36, 42, 48),
                         include.lowest = T)) %>%
  group_by(sub_id, trial_type) %>%
  summarize(prop_correct = mean(correct))

summary_stats <- unique(mss$trial_type) %>%
  map_df(get_mes_by_group, mss)
```


```{r glm_model1, cache = FALSE}
crit_sample <- final_sample  %>%
    filter(trial_type != "FF") %>%
    mutate(condition_type = ifelse(trial_type %in% c("NF", "NN"),
                                 "N", "F"),
           trial_type2 = ifelse(trial_type %in% c("NN", "C-NN"),
                                 "NN", "NF"),
           age_months = scale(age_months, 
                              scale = FALSE, center = TRUE), # scale continuous analysis
           prop_correct_vocab = scale(prop_correct_vocab, 
                                      scale = FALSE, center = TRUE))

accuracy_model1 <- glmer(correct ~ condition_type * trial_type2 +
                  (condition_type | sub_id) +
                  (trial_type2 | sub_id),
                  family = "binomial",
                  data = crit_sample,
                  control = glmerControl(optimizer = "bobyqa"))
m1 <- summary(accuracy_model1) 

print_model1 <- m1$coefficients %>%
  data.frame() %>%
  stats::setNames(c("B", "SE", "Z", "p")) %>%
  rownames_to_column("term")  %>%
  mutate_at(2:5, function(x) {round(x, 2)}) %>%
  mutate(p_print = ifelse(p == 0, "< .001", paste0("= ", p)),
         pretty_model = paste0(B, ", _SE_ = ", SE, 
                               ", _Z_ = ", Z, ", _p_ ", p_print))
```

We next examined performance on the four trial types. Children were above chance (.5) in both types of control conditions where they were asked to identify a familiar referent (Control-NF: `r summary_stats %>% filter(condition == "C-NF") %>% pull(tidy_print)`; Control-NN:  `r summary_stats %>% filter(condition == "C-NN") %>% pull(tidy_print)`). Critically, children also succeeded on both types of  experimental trials where they were required to select the novel object  (NF: `r summary_stats %>% filter(condition == "NF") %>% pull(tidy_print)`; NN: `r summary_stats %>% filter(condition == "NN") %>% pull(tidy_print)`; see Appendix B for task reliability). 

To compare all four conditions, we fit a model predicting accuracy with target type (F (Control) vs. N (Experimental)) and trial type (NF vs. NN) as fixed effects.\footnote{The model specification was as follows:  \texttt{accuracy  $\sim$ target.type $*$ trial.type  + (target.type~\textbar~subject.id)  +   (trial.type~\textbar~subject.id)}.} There was a main effect of trial type, suggesting that participants were less accurate in NN  trials compared to NF trials ($\beta$ = `r print_model1 %>% filter(term == "trial_type2NN") %>% pull(pretty_model)`). There was also a marginal main effect of target type, with novel referents being more difficult for children than familiar referents  ($\beta$ = `r print_model1 %>% filter(term == "condition_typeN") %>% pull(pretty_model)`). Finally, there was a marginal interaction between the two factors ($\beta$ =  `r print_model1 %>% filter(term == "condition_typeN:trial_type2NN") %>% pull(pretty_model)`), suggesting that Novel target trials (Experimental) were more difficult than Familiar target trials (Control) for NF trials but not NN trials.

```{r, age_vocab_corr}
age_vocab_corr <- final_sample %>%
  distinct(sub_id, .keep_all = T) %>%
  select(sub_id, age_months, prop_correct_vocab) %>%
  do(tidy(cor.test(.$age_months, .$prop_correct_vocab))) %>%
  mutate(pretty_corr = get_pretty_mean(estimate, 
                                  conf.low, 
                                  conf.high))
```

Our main question was how accuracy on the experimental trials changed over development. We examined two measures of developmental change: Age (months) and vocabulary size, as measured in our vocabulary assessment. We assigned a vocabulary score to each child as the proportion of correct selections on the vocabulary assessment out of 20 possible. Age and vocabulary size were positively correlated, with older children tending to have larger vocabularies compared to younger children (*r* = `r age_vocab_corr$pretty_corr`, *p* < .001).

```{r trial_means_plot, fig.width=8, fig.height =3.5, fig.pos = "!t", fig.cap = "Accuracy data for four trial types by half-year age bins. Blue corresponds to trials with the canonical novel-familiar paradigm, and red corresponds to trials with two novel alternatives, where a novel of label for one of the objects is unambiguously introduced on a previous trial. The dashed line corresponds to chance. Ranges are 95\\% CIs.", include = F}

mss <- final_sample %>%
  mutate(age_group = cut(age_months, 
                         breaks = c(24, 30, 36, 42, 48),
                         include.lowest = T)) %>%
  group_by(sub_id, trial_type, age_group) %>%
  summarize(prop_correct = mean(correct))

ms <- mss %>%
    group_by(age_group, trial_type) %>%
    multi_boot_standard("prop_correct") %>%
    ungroup() %>%
    filter(trial_type != "FF") %>%
    mutate(condition_type = ifelse(trial_type %in% c("NF", "NN"),
                                 "Experimental\n (novel target)", 
                                 "Control \n (familiar target)"),
           trial_type2 = ifelse(trial_type %in% c("NN", "C-NN"),
                                 "Novel-Novel", "Novel-Familiar"),
           age_group = fct_recode(age_group, "2-2.5" = "[24,30]",
                                  "2.5-3" = "(30,36]",
                                  "3-3.5" = "(36,42]",
                                  "3.5-4" = "(42,48]")) 

ggplot(ms, aes(x = age_group,
               y = mean)) +
  facet_wrap(~condition_type) +
  geom_pointrange(aes(ymin = ci_lower, 
                      ymax = ci_upper, color = trial_type2), size = .5) +
  geom_line(aes(color = trial_type2, group = trial_type2)) +
  ylab("Proportion Correct") +
  xlab("Age (years)") +
  geom_hline(aes(yintercept = .5), lty = 2) +
  ylim(.4, 1) +
  theme_classic(base_size = 20) +
  #theme(strip.text = element_text(size = 11)) +
  ggthemes::scale_color_solarized(guide = 
                                    guide_legend(title = "Trial Type"))
```


```{r fig.width=8, fig.height = 4, fig.pos = "!t", fig.cap = "Accuracy as a function of age (months; left) and vocabulary size (proportion correct on vocabulary assessment; right) in Experiment 1. Blue corresponds to trials with the canonical novel-familiar ME task, and red corresponds to trials with two novel alternatives, where a novel of label for one of the objects is unambiguously introduced on a previous trial. The dashed line corresponds to chance. Ranges correspond to standard error."}

plot_data <- final_sample %>%
  filter(trial_type %in% c("NN", "NF")) %>%
  select(sub_id, prop_correct_vocab, correct, age_months, trial_type)  %>%
  group_by(sub_id, trial_type, prop_correct_vocab, age_months) %>%
  summarize(prop_correct = mean(correct)) %>%
  ungroup()

age_plot <- ggplot(plot_data, aes(x = age_months,
             y = prop_correct, 
             color = trial_type)) +
  geom_smooth(method = "lm", formula = y~log(x), alpha = .2) + 
  ylab("Proportion Correct in ME task") +
  geom_hline(aes(yintercept = .5), lty = 2) +
  theme_classic(base_size = 13) +
  ggtitle("Experiment 1: Age" ) +
  xlab("Age (Months)") +
  ggthemes::scale_color_solarized() +
  theme(legend.position = "none") +
  scale_x_continuous(breaks = c(24, 27, 30, 33, 36, 39, 42, 45, 48)) +
  ylim(0,1) 

vocab_plot <- ggplot(plot_data, aes(x = prop_correct_vocab,
             y = prop_correct, 
             color = trial_type)) +
  geom_smooth(method = "lm", formula = y~log(x), alpha = .2) + 
  ylab("Proportion Correct in ME task") +
  ggtitle("Experiment 1: Vocabulary Size" ) +
  xlab("Proportion Correct in Vocabulary Test") +
  geom_hline(aes(yintercept = .5), lty = 2) +
  theme_classic(base_size = 13) +
  ggthemes::scale_color_solarized(guide = 
                                    guide_legend(title = "Trial Type")) +
  scale_x_continuous(breaks = c(24, 27, 30, 33, 36, 39, 42, 45, 48)) +
  ylim(0,1) +
  xlim(0,1) +
  theme(legend.position = c(.8, .2))

multiplot(age_plot, vocab_plot, cols = 2)
```

```{r glm_analysis, cache = FALSE}
## FREQUENTIST - this is the preregistered model (https://osf.io/tt29f/register/5771ca429ad5a1020de2872e)
accuracy_model2 <- glmer(correct ~ prop_correct_vocab * trial_type2 * age_months +
                  (trial_type2 | sub_id),
                  family = "binomial",
                  data = filter(crit_sample,
                                condition_type == "N"),
                  control = glmerControl(optimizer = "bobyqa"))
m2 <- summary(accuracy_model2)

print_model2 <- m2$coefficients %>%
  data.frame() %>%
  stats::setNames(c("Beta", "SE", "Z", "p")) %>%
  rownames_to_column("raw_term")  %>%
  mutate_if(is.numeric, function(x){round(x, 2)}) %>%
  ungroup()%>%
  mutate(p = ifelse(p == 0, "<.0001", p),
         term = c("(Intercept)",
                  "Vocabulary",
                  "Trial Type (NN)",
                  "Age",
                  "Vocabulary x Trial Type (NN)",
                  "Vocabulary x Age",
                  "Age x Trial Type (NN)",
                  "Vocabulary x Age x Trial Type (NN)")) %>%
  select(term, everything(), -raw_term)

text_pretty_model2 <- print_model2 %>%
  mutate(p = ifelse(p == "<.0001", p,  paste0("= ", p)),
         pretty_model = paste0( Beta, ", _SE_ = ", SE,
                               ", _Z_ = ", Z, ", _p_ ", p))
```

```{r, results = "asis"}
apa_table(print_model2, caption ="Parameters of logistic mixed model predicting accuracy on ME trials as a function of trial type (Novel-Familiar (NF) vs. Novel-Novel (NN)), age (months), and vocabulary size as measured by our vocabulary assessment.")
```

Figure 4 shows log linear model fits for accuracy as a function of age (left) and vocabulary size (right) for both NF and NN trial types.  To examine the relative influence of maturation and vocabulary size on accuracy, we fit a model predicting accuracy with vocabulary size, age, and trial type (Experimental-NN and Experimental-NF).\footnote{The model specification was as follows:  \texttt{accuracy  $\sim$ vocabulary.size  $*$ age $*$ trial.type + (trial.type~\textbar~subject.id)}} Table 4 presents the model parameters. The only reliable predictor of accuracy was vocabulary size ($\beta$ = `r text_pretty_model2 %>% filter(term == "Vocabulary") %>% select(pretty_model) %>% pull()`), suggesting that children with larger vocabularies tended to be more accurate in the ME task. Vocabulary size did not interact with trial type ($\beta$ = `r text_pretty_model2 %>% filter(term == "Vocabulary x Trial Type (NN)") %>% select(pretty_model) %>% pull()`), suggesting that children with larger vocabularies were more likely to make the ME inference in both NF and NN trials. Notably, age was not a reliable predictor of accuracy over and above vocabulary size ($\beta$ = `r text_pretty_model2 %>% filter(term == "Age") %>% select(pretty_model) %>% pull()`). 

### Discussion
Experiment 1 examines the relationship between the strength of the ME effect and vocabulary size. We find that the strength of the ME effect is highly predicted by vocabulary size, with children with larger vocabularies tending to show a larger ME effect. In addition, we find that the bias is larger for NF trials, compared to NN trials. 


```{r fig.height = 4.5, fig.pos = "!t", fig.cap = "Meta-analytic data and data from experimental trials in Experiment 1 as a function of age. Effect sizes for Experiment 1 data are calculated for each participant, assuming the across-participant mean standard deviation as an estimate of the participant level standard deviation. Ranges correspond to standard error."} 

TYPICAL_MA_DATA_PATH <- here("data/0_metaanalysis_data_typical_only.csv")
meta_plot_data <- read_csv(TYPICAL_MA_DATA_PATH) %>%
  select(-mean_production_vocab) %>%
  mutate(data_source = "Meta-analysis",
         trial_type = fct_recode(trial_type, NF = "FN")) 

sds <- final_sample %>%
  filter(trial_type %in% c("NN", "NF")) %>%
  group_by(trial_type) %>%
  summarize(overall_sd = sd(correct))

exp_plot_data <- final_sample %>%
  filter(trial_type %in% c("NN", "NF")) %>%
  select(sub_id, prop_correct_vocab, correct, age_months, trial_type)  %>%
  left_join(sds, by ="trial_type") %>%
  group_by(sub_id, trial_type, prop_correct_vocab, age_months, overall_sd) %>%
  summarize(prop_correct = mean(correct),
            d_calc = (prop_correct - .5)/overall_sd[1]) %>%
  ungroup()%>%
  select(d_calc, age_months, trial_type) %>%
  mutate(data_source =  "Experiment 1") 

exp1_meta_data <- meta_plot_data %>%
  bind_rows(exp_plot_data)  %>%
  filter(age_months >= 24 & age_months <= 48) 

exp1_meta_data %>%
  ggplot(aes(x = age_months,
             y = d_calc, 
             color = trial_type, linetype = data_source)) +
  geom_hline(aes(yintercept = 0)) +
  geom_smooth(method = "lm", formula = y~log(x), alpha = .2) + 
  ylab("Effect size (d)") +
  theme_classic(base_size = 13) +
  ggtitle("Meta-analytic and Experiment 1 Effect Sizes" ) +
  scale_x_continuous(breaks = c(24, 27, 30, 33, 36, 39, 42, 45, 48)) +
  scale_linetype_manual(name = "Data Source", values = c("solid", "dashed")) +
  xlab("Age (Months)") +
  ggthemes::scale_color_solarized(guide = 
                                    guide_legend(title = "Trial Type"))
```

The pattern of findings is broadly consistent with meta-analytic estimates of those same effects. Figure 5 presents the data from the experimental conditions in Experiment 1 together with meta-analytic estimates, as a function of age. To compare the experimental data with the meta-analytic data, an effect size was calculated for each participant.\footnote{Because some participants had no variability in their responses (all correct or all incorrect), we used the across-participant mean standard deviation as an estimate of the participant level standard deviation in order to convert accuracy scores into Cohen's {\it d} values.} As in the meta-analytic models, the effect size is smaller for NN trials compared to NF trials, though the magnitude of this difference is smaller.  The experimental data thus provide converging evidence with the meta-analysis that there is developmental change in the strength of the bias, and that the effect is weaker for NN trials.

There are, however, some notable differences between the Experiment 1 data and the meta-analytic results. First, while the direction of the influence of age on the ME effect is the same in both studies, the magnitude of the developmental effect is much smaller in Experiment 1 relative to the meta-analytic data within the same 24- to 48- month developmental range. This difference could be due to the fact that researchers in the meta-analytic studies calibrate their method to the age of their participants (e.g., eye-tracking for younger children and pointing for older children), and there is evidence that different methods produce effect sizes of varying sizes across development
[@bergmann2018promoting]. Second, the variance is larger for the meta-analytic estimates compared to the experimental data, presumably because there is more heterogeneity across experiments than across participants within the same experiment.  Third, the magnitude of the  effect of trial type (NF vs. NN) is much smaller in the experimental data, relative to the meta-analytic data. This incongruence between the experimental and meta-analytic results could be due to any number of differences across studies (e.g. differences in the difficulty of the familiar word in NF paradigms).

In addition, the data from Experiment 1  provide new evidence relevant to the mechanism underlying the effect: children with larger vocabulary tend to have a stronger ME bias. In principle there are two ways that vocabulary knowledge could support the ME inference. The first is by influencing the strength of the learner's knowledge about the label for the familiar word: If a learner is more certain about the label for the familiar object, they can be more certain about the label for novel object. This account explains  the developmental change observed for NF trials. However, this account does not explain the relationship of vocabulary with NN trials, since no prior vocabulary knowledge is directly relevant to this inference.  The relationship between vocabulary size and the magnitude of the effect in NN trials suggests that vocabulary knowledge could also influence the effect by providing evidence for general constraint that there is a one-to-one mapping between words and referents. Another possibility is that the observed correlation between vocabulary size and ME could be due to a third variable, such as lexical processing abilities [@White_2008; @Merriman_1995; @Merriman_1991; @mather2011mutual].  It seems likely that some or most of ME developmental change is carried by more general developmental change and a challenge for researchers is parse out the relative contribution of different developmental changes on the ME effect.

Regardless of the specific route through which vocabulary knowledge influences the ME inference, the hypothesized relationship between experience and the ME effect is causal. Nevertheless, the data from both the meta-analytic study and the current experiment only provide correlational evidence about their relationship. In Experiment 2, we aimed to more directly test the causal hypothesis by experimentally manipulating the strength of the learner's knowledge about the familiar word-object mapping.

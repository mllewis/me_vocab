# Experiment 1: ME and Vocabulary
The goal of Experiment 1 is to more directly explore the influence of vocabulary-related language experience on the disambiguation inference. Our meta-analysis points to a robust developmental increase in the strength of the disambiguation effect with age. While all four accounts are able to predict this change, only the overhypothesis account predicts that this increase should be related directly related to vocabulary knowledge. In our meta-analytic analysis, we explored the relationship  between vocabulary size and the magnitude of the disambiguation effect in the prior literature, but this analysis is limeted by the fact that vocabulary size is not measured for most studies in our sample. In Experiment 1, we therefore aimed to test the prediction that children with larger vocabularies should have a stronger disambiguation bias by measuring vocabulary size on a large sample of children who completed the disambiguation task. Consistent with the overhypothesis account, we find X.

## Methods

### Participants

```{r read_in_data}
exp1 <- read_csv("../../exp1/processed/all_exp1_data_complete.csv") %>%
  select(sub_id, age_months, gender, english, 
         exclude2, prop_correct_vocab, trial_type, correct, 
         start_time, end_time, resp_start_time, object1) 
```

```{r do_exclusions}
# number of trials age [24, 48]
good_age <- exp1 %>%
  distinct(sub_id, .keep_all = T) %>%
  filter(age_months >= 24 & age_months <= 48) # 213

# english input >= 75
good_language <- exp1 %>%
  distinct(sub_id, .keep_all = T) %>%
  filter(english >= 75) # 189 

# prop_correct C-NF > .5
good_controls <- exp1 %>%
  filter(trial_type == "C-NF") %>%
  group_by(sub_id) %>%
  summarize(prop_correct = sum(correct) / n())  %>%
  filter(prop_correct >= .5) # 171

# completed all trials
NUM_TRIALS <- 19
good_counts <- exp1 %>%
  count(sub_id) %>%
  filter(n == NUM_TRIALS) # 160

good_sub_ids <- list(good_counts$sub_id, 
                     good_age$sub_id,
                     good_language$sub_id,
                     good_controls$sub_id) %>%
  accumulate(intersect) %>%
  last()

final_sample <- filter(exp1, sub_id %in% good_sub_ids) 
num_good <- length(unique(final_sample$sub_id))
num_total <- length(unique(exp1$sub_id))

gender_counts <- final_sample %>%
  distinct(sub_id, .keep_all = T) %>%
  count(gender)
```

A sample of `r num_total` children were recruited at the Childrenâ€™s Discovery Museum of San Jose.   `r num_total - num_good` children were excluded because they did not satsify our planned inclusion criteria:  within the age range of 24-48 months (*n* = `r num_total - dim(good_age)[1]`), completed all trials (*n* = `r num_total - dim(good_counts)[1]`), exposed to English greater than 75% of the time (*n* = `r num_total - dim(good_language)[1] `), and correctly answered at least half of the familiar noun control trials  (*n* = `r num_total - dim(good_controls)[1]`). Our final sample included `r num_good` children (*N*~females~ = `r filter(gender_counts, gender == "female") %>% select(n) %>% unlist(use.names = F)`).

### Stimuli
The disambiguation task included color pictures of 14 novel objects (e.g., a pair of tongs) and 24 familiar objects (e.g. a cookie; see SI). Items in the vocabulary assessment were a fixed set of 20 developmentally appropriate words from the Pearson Peabody Vocabulary Test [@dunn1965peabody]. The novel words were XXX.

### Design and Procedure
Sessions took place individually in a small testing room away from the museum floor.  The experimenter first introduced the child to "Mr. Fox," a cartoon character who wanted to play a guessing game. The experimenter explained that Mr. Fox would tell them the name of the object they had to find, so they had to listen carefully. Children then completed a series of 19 trials on an iPad, 3 practice trials followed by 16 experimental trials. In the practice trials, children were shown two familiar pictures (FF) on the iPad and asked to select one, given a label. If the participant chose incorrectly on a practice trial, the audio would correct them and allow the participant to choose again.

The child then completed the test phase. Like the practice trials, each of the test trials consisted of a word and two pictures, and the child's task was to identify the referent. Within participants, we manipulated two features of the task: the target referent (Novel (Experimental)  or Familiar (Control)) and the type of alternatives (Novel-Familiar or Novel-Novel; NF or NN). On novel referent trials, children were given a novel word and expected to select the novel object via the disambiguation inference. On familiar referent trials, children were given a familiar word and expected to select the correct familiar object. On Novel-Novel trials, children saw pictures of two novel objects (e.g.\ tongs and cookie) [How were N words introduced for NN trials?]. On Novel-Familiar trials, children saw a picture of a novel object and a familiar objects (e.g. a leak and tongs). The design features were fully crossed such that half of the trials were of each trial type  (Experimental-NF, Experimental-NN, Control-NF, Control-NN). Trials were presented randomly, and children were only allowed to make one selection. 

After the disambiguation task, we measured children's vocabulary in a simple vocabulary assessment. In the assessment, children were presented with four randomly selected images, and prompted to choose a picture given a label. Children completed 2 practice trials followed by 20 test trials.  

### Data analysis
All models are generalized linear mixed-effect models fit with the lme4 package in R [@R-lme4]. Each model was fit with the maximal random effect structure. All ranges are 95% confidence intervals. Effect sizes are Cohen's *d*s.

## Results and Discussion
```{r practice_trials}
practice_trials <- final_sample %>%
  filter(trial_type == "FF") %>%
  group_by(sub_id) %>%
  summarize(prop_correct = mean(correct)) %>%
  mutate(group = "temp") %>%
  group_by(group) %>%
  multi_boot_standard("prop_correct") 

print_practice <- get_pretty_mean(practice_trials$mean, 
                                  practice_trials$ci_lower, 
                                  practice_trials$ci_upper)
```

Participants completed the three practice trials (FF) with high accuracy, suggesting that they understood the task (*M* = `r print_practice`). 

```{r summary_stats}
mss <- final_sample %>%
  mutate(age_group = cut(age_months, 
                         breaks = c(24, 30, 36, 42, 48),
                         include.lowest = T)) %>%
  group_by(sub_id, trial_type) %>%
  summarize(prop_correct = mean(correct))

summary_stats <- unique(mss$trial_type) %>%
  map_df(get_mes_by_group, mss)
```


```{r glm_model1, cache = T}
crit_sample <- final_sample  %>%
    filter(trial_type != "FF") %>%
    mutate(condition_type = ifelse(trial_type %in% c("NF", "NN"),
                                 "N", "F"),
           trial_type2 = ifelse(trial_type %in% c("NN", "C-NN"),
                                 "NN", "NF"),
           age_months = scale(age_months), # scale continious analysis
           prop_correct_vocab = scale(prop_correct_vocab)) 


accuracy_model1 <- glmer(correct ~ condition_type * trial_type2 +
                  (trial_type2*condition_type | sub_id) +
                    (trial_type2*condition_type | object1), 
                  family = "binomial",
                  data = crit_sample,
                  control = glmerControl(optimizer = "bobyqa"))
m1 <- summary(accuracy_model1) 

print_model1 <- m1$coefficients %>%
  data.frame() %>%
  stats::setNames(c("B", "SE", "Z", "p")) %>%
  rownames_to_column("term")  %>%
  mutate_at(2:5, function(x) {round(x, 2)}) %>%
  mutate(p_print = ifelse(p == 0, "< .001", paste0("= ", p)),
         pretty_model = paste0("B = ", B, ", SE = ", SE, 
                               ", Z = ", Z, ", p ", p_print))
```

We next examined performance on the four trial types. Children were above chance (.5) in both types of control conditions where they were asked to identify a familiar referent (Control-NF: `r summary_stats %>% filter(condition == "C-NF") %>% pull(tidy_print)`; Control-NN:  `r summary_stats %>% filter(condition == "C-NN") %>% pull(tidy_print)`). Critically, children also succeeded on both types of  experimental trials where they were required to select the novel object by making a disambiguation inference (NF: `r summary_stats %>% filter(condition == "NF") %>% pull(tidy_print)`; NN: `r summary_stats %>% filter(condition == "NN") %>% pull(tidy_print)`). 

To compare these four conditions, we fit a model predicting accuracy with target type (F (Control) vs. N (Experimental)) and trial type (NF vs. NN) as fixed effects. We included both target type and trial type as main effects as well as a term for their interaction. There was a main effect of trial type, suggesting that participants were less accurate in NN  trials compared to NF trials (`r print_model1 %>% filter(term == "trial_type2NN") %>% pull(pretty_model)`). The main effect of target type was not sigificant (`r print_model1 %>% filter(term == "condition_typeN") %>% pull(pretty_model)`). The interaction between the two factors was marginal (`r print_model1 %>% filter(term == "condition_typeN:trial_type2NN") %>% pull(pretty_model)`), suggesting that Novel target trials (Experimental) were more difficult that Familiar target trials (Control) only NF trials. 

```{r, age_vocab_corr}
age_vocab_corr <- final_sample %>%
  distinct(sub_id, .keep_all = T) %>%
  select(sub_id, age_months, prop_correct_vocab) %>%
  do(tidy(cor.test(.$age_months, .$prop_correct_vocab))) %>%
  mutate(pretty_corr = get_pretty_mean(estimate, 
                                  conf.low, 
                                  conf.high))
```

Our main question was how accuracy on the experimental trials changed over development. We examined two measures of developmental change: Age (months) and vocabulary size, as measured in our vocabulary assesessment. We assigned a vocabulary score to each child as the proportion correct selections on the vocabulary assessment out of 20 possible. Age and vocabulary size were positively correlated, with older children tending to have larger vocabularies (*r* = `r age_vocab_corr$pretty_corr`, *p* < .001).

```{r trial_means_plot, fig.pos = 'T!', fig.width=8, fig.height =3.5, fig.cap = "Accuracy data for four trial types by half-year age bins. Blue corresponds to trials with the canonical novel-familiar paradigm, and red corresponds to trials with two novel alternatives, where a novel of label for one of the objects is unambiguously introduced on a previous trial. The dashed line corresponds to chance. Ranges are 95\\% CIs.", include = F}

mss <- final_sample %>%
  mutate(age_group = cut(age_months, 
                         breaks = c(24, 30, 36, 42, 48),
                         include.lowest = T)) %>%
  group_by(sub_id, trial_type, age_group) %>%
  summarize(prop_correct = mean(correct))

ms <- mss %>%
    group_by(age_group, trial_type) %>%
    multi_boot_standard("prop_correct") %>%
  ungroup() %>%
    filter(trial_type != "FF") %>%
    mutate(condition_type = ifelse(trial_type %in% c("NF", "NN"),
                                 "Experimental\n (novel target)", 
                                 "Control \n (familiar target)"),
           trial_type2 = ifelse(trial_type %in% c("NN", "C-NN"),
                                 "Novel-Novel", "Novel-Familiar"),
           age_group = fct_recode(age_group, "2-2.5" = "[24,30]",
                                  "2.5-3" = "(30,36]",
                                  "3-3.5" = "(36,42]",
                                  "3.5-4" = "(42,48]")) 

ggplot(ms, aes(x = age_group,
               y = mean)) +
  facet_wrap(~condition_type) +
  #geom_bar(stat = "identity", position= position_dodge(width=0.9))+
  #geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), 
   #              position= position_dodge(width=0.9)) +
  geom_pointrange(aes(ymin = ci_lower, 
                      ymax = ci_upper, color = trial_type2), size = .5) +
  geom_line(aes(color = trial_type2, group = trial_type2)) +
  ylab("Accuracy") +
  xlab("Age (years)") +
  geom_hline(aes(yintercept = .5), lty = 2) +
  ylim(.4, 1) +
  theme_classic() +
  theme(strip.text = element_text(size = 11)) +
  ggthemes::scale_color_solarized(guide = 
                                    guide_legend(title = "Trial Type"))
```

```{r dev_change_plot, fig.pos = 'T!', fig.width=8, fig.height = 4.5, fig.cap = "Accuracy as a function of age (months; left) and vocabulary size (proportion correct on vocabulary assessment; right). Blue corresponds to trials with the canonical novel-familiar disambiguation paradigm, and red corresponds to trials with two novel alternatives, where a novel of label for one of the objects is unambiguously introduced on a previous trial. The dashed line corresponds to chance. Ranges are 95\\% CIs."}
# mss <- final_sample %>%
#   mutate(age_group = cut(age_months, 
#                          breaks = seq(24, 48, by = 1),
#                          include.lowest = T),
#          age_month = floor(age_months)) %>%
#   group_by(trial_type, age_month) %>%
#   filter(trial_type %in% c("NF", "NN")) %>%
#   summarize(prop_correct = mean(correct)) %>%
#   ungroup()
# 
# ggplot(mss, aes(x = age_month,
#                 y = prop_correct, color = trial_type)) +
#   geom_point() +
#   geom_smooth(method = "lm", alpha = .2) + # formulate = y~log(x)
#   ylab("Accuracy") +
#   xlab("Age (years)") +
#   geom_hline(aes(yintercept = .5), lty = 2) +
#   ylim(.4, 1) +
#   theme_classic() +
#   theme(strip.text = element_text(size = 11)) +
#   ggthemes::scale_color_solarized(guide = 
#                                     guide_legend(title = "Trial Type"))

# mss_vocab <- final_sample %>%
#   mutate(vocab_group = cut(prop_correct_vocab, 
#                          breaks = 7,
#                          include.lowest = T)) %>%
#   group_by(trial_type, vocab_group) %>%
#   filter(trial_type %in% c("NF", "NN")) %>%
#   summarize(prop_correct = mean(correct)) %>%
#   ungroup()

plot_data <- final_sample %>%
  filter(trial_type %in% c("NN", "NF")) %>%
  select(sub_id, prop_correct_vocab, correct, age_months, trial_type) 
  #gather("dv", "value", c(-correct, -sub_id, -trial_type)) 

age_plot <- ggplot(plot_data, aes(x = age_months,
             y = correct, 
             color = trial_type)) +
  geom_smooth(method = "lm", formula = y~log(x), alpha = .2) + 
  ylab("Accuracy") +
  geom_hline(aes(yintercept = .5), lty = 2) +
  theme_classic() +
  ggtitle("Age" ) +
  xlab("Age (Months)") +
  ggthemes::scale_color_solarized() +
  theme(legend.position = "none") +
  ylim(0,1) 

vocab_plot <- ggplot(plot_data, aes(x = prop_correct_vocab,
             y = correct, 
             color = trial_type)) +
  geom_smooth(method = "lm", formula = y~log(x), alpha = .2) + 
  ylab("Accuracy") +
   ggtitle("Vocabulary Size" ) +
  xlab("Prop. Correct Words") +
  geom_hline(aes(yintercept = .5), lty = 2) +
  theme_classic() +

  ggthemes::scale_color_solarized(guide = 
                                    guide_legend(title = "Trial Type")) +
  ylim(0,1) 

multiplot(age_plot, vocab_plot, cols = 2)

```

```{r glm_analysis, cache = T}

accuracy_model2 <- glmer(correct ~ prop_correct_vocab * trial_type2 * age_months +
                  (trial_type2 | sub_id) +  
                    (trial_type2 | object1), 
                  family = "binomial",
                  data = filter(crit_sample, 
                                condition_type == "N"),
                  control = glmerControl(optimizer = "bobyqa"))
m2 <- summary(accuracy_model2)


print_model2 <- m2$coefficients %>%
  data.frame() %>%
  stats::setNames(c("Beta", "SE", "Z", "p")) %>%
  rownames_to_column("raw_term")  %>%
  mutate_if(is.numeric, function(x){round(x, 2)}) %>%
  mutate(p = ifelse(p == 0, "<.0001", p), 
         term = c("(Intercept)",
                  "Vocabulary", 
                  "Trial Type (NN)",
                  "Age", 
                  "Vocabulary x Trial Type (NN)",
                  "Vocabulary x Age",
                  "Age x Trial Type (NN)",
                  "Vocabulary x Age x Trial Type (NN)")) %>%
  select(term, everything(), -raw_term) 

  
kable(print_model2, 
        row.names = F, 
        format = "latex", 
        booktabs = TRUE)  %>%
    kable_styling(font_size = 8)

text_pretty_model2 <- print_model2 %>%
  mutate(p = ifelse(p == "<.0001", p,  paste0("= ", p)),
         pretty_model = paste0("B = ", Beta, ", SE = ", SE, 
                               ", Z = ", Z, ", p ", p))
```

Figure \ref{fig:dev_change_plot} shows log linear model fits for accuracy as a function of age (left) and vocabulary size (right) for both NF and NN trial types.  To examine the relative influence of maturation and vocabulary size on accuracy, we fit a model predicting accuracy with vocabulary size, age, and trial type (Experimental-NN, and Experimental-NF). We included all possible main and interaction terms as fixed effects. Table 1 presents the model parameters. The only reliable predictor of accuracy was vocabulary size (`r text_pretty_model2 %>% filter(term == "Vocabulary") %>% select(pretty_model) %>% pull()`)), suggesting that children with larger vocabularies tended to be more accurate in the disambiguation task. Notably, age was not a reliable predictor of accuracy over and above vocabulary size (`r text_pretty_model2 %>% filter(term == "Age") %>% select(pretty_model) %>% pull()`). 

### Discussion
Could be specific strength of particular word in the NF pairing

but we also get it for NN trials alone

compare to ME es

